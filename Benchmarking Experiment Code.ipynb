{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "#sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import fbeta_score,recall_score, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#imblearn\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import (\n",
    "                                    RandomOverSampler, \n",
    "                                    SMOTE, \n",
    "                                    ADASYN,\n",
    "                                    BorderlineSMOTE\n",
    "                                   )\n",
    "from imblearn.under_sampling import (\n",
    "                                    RandomUnderSampler,\n",
    "                                    OneSidedSelection,\n",
    "                                    NeighbourhoodCleaningRule,\n",
    "                                    TomekLinks\n",
    "                                   )\n",
    "from imblearn.combine import (SMOTEENN, SMOTETomek)\n",
    "#others\n",
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import categorical_embedder as cem\n",
    "import collections\n",
    "import category_encoders as ce\n",
    "from matplotlib import pyplot as plt\n",
    "import category_encoders as ce\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "import math\n",
    "\n",
    "\n",
    "#instantiate scaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_mode_impute(df):\n",
    "    impute_mean = SimpleImputer(strategy=\"mean\")\n",
    "    impute_mc = SimpleImputer(strategy=\"most_frequent\")\n",
    "    column_list = df.columns[df.isna().any()].tolist()\n",
    "    if column_list == []:\n",
    "#        print('No columns with missing values found')\n",
    "        return df\n",
    "    for column in column_list:\n",
    "         if is_numeric_dtype(df[column]):\n",
    "            impute_mean = impute_mean.fit(df[[column]])\n",
    "            df[column] = impute_mean.transform(df[[column]]).ravel()\n",
    "         elif is_string_dtype(df[column]):\n",
    "            impute_mc = impute_mc.fit(df[[column]])\n",
    "            df[column] = impute_mc.transform(df[[column]]).ravel()\n",
    "         else:\n",
    "            print('error')  \n",
    "    return df \n",
    "            \n",
    "def woe_encode(x,y):\n",
    "    column_list = x.columns.tolist()\n",
    "    for column in column_list:\n",
    "        if is_string_dtype(x[column]):\n",
    "            encoder = ce.WOEEncoder(cols=[column])\n",
    "            encoder.fit(x,y)\n",
    "            x = encoder.transform(x)\n",
    "    X_woe['BAD'] = y_woe\n",
    "    ld_woe = X_woe\n",
    "    return ld_woe\n",
    "\n",
    "def one_hot_encode(df):\n",
    "    column_list = df.columns.tolist()\n",
    "    for column in column_list:\n",
    "        if is_string_dtype(df[column]):\n",
    "            one_hot_column = pd.get_dummies(df[column],prefix=column,drop_first=True)\n",
    "            df = df.drop(column,axis = 1)\n",
    "            df = df.join(one_hot_column)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(X_train,y_train,X_test,y_test,resampler,parameterR, transformer, parameterT,encoder,classifier):\n",
    "    \n",
    "    res = []\n",
    "    model_name = type(classifier).__name__\n",
    "    model = classifier.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    #print(y_proba)\n",
    "    y_proba = y_proba[:, 1]\n",
    "    y_test_arr = np.array(y_test)\n",
    "    rec = str(round(recall_score(y_test_arr, y_pred),4))\n",
    "    f1 = str(round(metrics.f1_score(y_test, y_pred),4))\n",
    "    auc = str(round(metrics.roc_auc_score(y_test, y_pred),4))\n",
    "    bs = str(round(metrics.brier_score_loss(y_test, y_proba),4))\n",
    "    print(dataset_name,encoder, resampler,parameterR,transformer,parameterT, model_name,rec,f1,auc,bs )\n",
    "    res.append(( dataset_name,encoder, resampler,parameterR,transformer,parameterT, model_name,rec,f1,auc,bs ))\n",
    "    return res\n",
    "\n",
    "\n",
    "def split(X,y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify = y)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "def classify_master(X_train, X_test, y_train, y_test,encoder_name,classifier):   \n",
    "    preprocess_technique_stat = pd.DataFrame()\n",
    "    \n",
    "    stat_encoded = classify(X_train,y_train,X_test,y_test,'NA','NA', 'NA', 'NA',encoder_name,classifier)\n",
    "    preprocess_technique_stat = preprocess_technique_stat.append(stat_encoded)\n",
    "    \n",
    "    stat_resample = resample_classify(X_train,X_test, y_train,y_test, ratios,encoder_name,classifier)\n",
    "    preprocess_technique_stat = preprocess_technique_stat.append(stat_resample)\n",
    "    \n",
    "    stat_reduce = reduce_classify(X_train,y_train,X_test,y_test,'NA', 'NA',encoder_name,classifier)\n",
    "    preprocess_technique_stat = preprocess_technique_stat.append(stat_reduce)\n",
    "    \n",
    "    stat_combined = process_classify(X_train,X_test, y_train,y_test, ratios,encoder_name,classifier)\n",
    "    preprocess_technique_stat = preprocess_technique_stat.append(stat_combined)\n",
    "\n",
    "    return preprocess_technique_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_classify(X_train,X_test, y_train,y_test, ratios,encoder,classifier):\n",
    "    results = pd.DataFrame()\n",
    "    k_neighbors = [5,10,15]\n",
    "\n",
    "    \n",
    "    resampler_instance = OneSidedSelection()\n",
    "    pipeline = Pipeline([('resampler', resampler_instance),  ('clf', classifier)])\n",
    "    params = {'resampler__sampling_strategy' : ['not minority','not majority','all'],\n",
    "              'resampler__n_neighbors':[1,3,6],\n",
    "             'resampler__n_seeds_S':[10,100,200]}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    grid.fit(X_train, y_train)\n",
    "    pRatio = grid.best_params_['resampler__sampling_strategy']\n",
    "    pNeighbors = grid.best_params_['resampler__n_neighbors']\n",
    "    pSeeds = grid.best_params_['resampler__n_seeds_S']\n",
    "    resampler_instance = OneSidedSelection(sampling_strategy=pRatio,n_neighbors=pNeighbors,n_seeds_S=pSeeds)\n",
    "    X_resampled, y_resampled = resampler_instance.fit_sample(X_train, y_train)\n",
    "    stat = reduce_classify(X_resampled,y_resampled,X_test,y_test,'OSS',grid.best_params_ ,encoder,classifier)\n",
    "    results = results.append(( stat ))\n",
    "    \n",
    "    \n",
    "    ROS_instance = RandomOverSampler()\n",
    "    pipeline = Pipeline([('resampler', ROS_instance), ('clf', classifier)])\n",
    "    params = {'resampler__sampling_strategy' : ratios}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    grid.fit(X_train, y_train)\n",
    "    pRatio = grid.best_params_['resampler__sampling_strategy']\n",
    "    ROS_instance = RandomOverSampler(sampling_strategy=pRatio)\n",
    "    X_resampled, y_resampled = ROS_instance.fit_sample(X_train, y_train)\n",
    "    stat = reduce_classify(X_resampled,y_resampled,X_test,y_test,'ROS', grid.best_params_,encoder,classifier)\n",
    "    results = results.append(( stat ))\n",
    "\n",
    "    \n",
    "    resampler_instance = SMOTE()\n",
    "    pipeline = Pipeline([('resampler', resampler_instance),  ('clf', classifier)])\n",
    "    params = {'resampler__sampling_strategy' : ratios,\n",
    "             'resampler__k_neighbors': k_neighbors}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    grid.fit(X_train, y_train)\n",
    "    pSS = grid.best_params_['resampler__sampling_strategy']\n",
    "    pKN = grid.best_params_['resampler__k_neighbors']\n",
    "    resampler_instance = SMOTE(sampling_strategy=pSS, k_neighbors = pKN)\n",
    "    X_resampled, y_resampled = resampler_instance.fit_sample(X_train, y_train)\n",
    "    stat = reduce_classify(X_resampled,y_resampled,X_test,y_test,'SMT', grid.best_params_,encoder,classifier)\n",
    "    results = results.append(( stat ))\n",
    "    \n",
    "    \n",
    "    resampler_instance = ADASYN()\n",
    "    pipeline = Pipeline([('resampler', resampler_instance), ('clf', classifier)])\n",
    "    params = {'resampler__sampling_strategy' : ratios,\n",
    "             'resampler__n_neighbors': k_neighbors}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    grid.fit(X_train, y_train)\n",
    "    pRatio = grid.best_params_['resampler__sampling_strategy']\n",
    "    pKN = grid.best_params_['resampler__n_neighbors']\n",
    "    resampler_instance = ADASYN(sampling_strategy=pRatio,n_neighbors=pKN)\n",
    "    X_resampled, y_resampled = resampler_instance.fit_sample(X_train, y_train)\n",
    "    stat = reduce_classify(X_resampled,y_resampled,X_test,y_test,'ADA', grid.best_params_,encoder,classifier)\n",
    "    results = results.append(( stat ))\n",
    "    \n",
    "    \n",
    "    resampler_instance = BorderlineSMOTE()\n",
    "    pipeline = Pipeline([('resampler', resampler_instance),  ('clf', classifier)])\n",
    "    params = {'resampler__sampling_strategy' : ratios,\n",
    "             'resampler__k_neighbors': k_neighbors,\n",
    "             'resampler__kind':['borderline-1','borderline-2']}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    grid.fit(X_train, y_train)\n",
    "    pSS = grid.best_params_['resampler__sampling_strategy']\n",
    "    pKN = grid.best_params_['resampler__k_neighbors']\n",
    "    pKnd = grid.best_params_['resampler__kind']\n",
    "    resampler_instance = BorderlineSMOTE(sampling_strategy=pSS,k_neighbors=pKN,kind=pKnd)\n",
    "    X_resampled, y_resampled = resampler_instance.fit_sample(X_train, y_train)\n",
    "    stat = reduce_classify(X_resampled,y_resampled,X_test,y_test,'SMT_BL',grid.best_params_,encoder,classifier)\n",
    "    results = results.append(( stat ))\n",
    "    \n",
    "    resampler_instance = SMOTEENN()\n",
    "    pipeline = Pipeline([('resampler', resampler_instance), ('clf', classifier)])\n",
    "    params = {'resampler__sampling_strategy' : ratios}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    grid.fit(X_train, y_train)\n",
    "    pRatio = grid.best_params_['resampler__sampling_strategy']\n",
    "    resampler_instance = SMOTEENN(sampling_strategy=pRatio)\n",
    "    X_resampled, y_resampled = resampler_instance.fit_sample(X_train, y_train)\n",
    "    stat = reduce_classify(X_resampled,y_resampled,X_test,y_test,'SMT_ENN', grid.best_params_,encoder,classifier)\n",
    "    results = results.append(( stat ))\n",
    "    \n",
    "    \n",
    "    resampler_instance = SMOTETomek()\n",
    "    pipeline = Pipeline([('resampler', resampler_instance),  ('clf', classifier)])\n",
    "    params = {'resampler__sampling_strategy' : ratios}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    grid.fit(X_train, y_train)\n",
    "    pRatio = grid.best_params_['resampler__sampling_strategy']\n",
    "    resampler_instance = SMOTETomek(sampling_strategy=pRatio)\n",
    "    X_resampled, y_resampled = resampler_instance.fit_sample(X_train, y_train)\n",
    "    stat = reduce_classify(X_resampled,y_resampled,X_test,y_test,'SMT_TL', grid.best_params_,encoder,classifier)\n",
    "    results = results.append(( stat ))\n",
    "    \n",
    "    \n",
    "    resampler_instance = RandomUnderSampler()\n",
    "    pipeline = Pipeline([('resampler', resampler_instance),  ('clf', classifier)])\n",
    "    params = {'resampler__sampling_strategy' : ratios}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    grid.fit(X_train, y_train)\n",
    "    pRatio = grid.best_params_['resampler__sampling_strategy']\n",
    "    resampler_instance = RandomUnderSampler(sampling_strategy=pRatio)\n",
    "    X_resampled, y_resampled = resampler_instance.fit_sample(X_train, y_train)\n",
    "    stat = reduce_classify(X_resampled,y_resampled,X_test,y_test,'RUS', grid.best_params_,encoder,classifier)\n",
    "    results = results.append(( stat ))\n",
    "    \n",
    "\n",
    "    resampler_instance = NeighbourhoodCleaningRule()\n",
    "    pipeline = Pipeline([('resampler', resampler_instance), ('clf', classifier)])\n",
    "    params = {'resampler__sampling_strategy' : ['not minority','not majority','all'],\n",
    "              'resampler__n_neighbors':[3,6,9]}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    grid.fit(X_train, y_train)\n",
    "    pRatio = grid.best_params_['resampler__sampling_strategy']\n",
    "    pN = grid.best_params_['resampler__n_neighbors']\n",
    "    resampler_instance = NeighbourhoodCleaningRule(sampling_strategy=pRatio,n_neighbors=pN)\n",
    "    X_resampled, y_resampled = resampler_instance.fit_sample(X_train, y_train)\n",
    "    stat = reduce_classify(X_resampled,y_resampled,X_test,y_test,'NCL',grid.best_params_,encoder,classifier)\n",
    "    results = results.append(( stat ))\n",
    "\n",
    "    \n",
    "    resampler_instance = TomekLinks()\n",
    "    pipeline = Pipeline([('resampler', resampler_instance),  ('clf', classifier)])\n",
    "    params = {'resampler__sampling_strategy' : ['not minority','not majority','all']}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    grid.fit(X_train, y_train)\n",
    "    pRatio = grid.best_params_['resampler__sampling_strategy']\n",
    "    resampler_instance = TomekLinks(sampling_strategy=pRatio)\n",
    "    X_resampled, y_resampled = resampler_instance.fit_sample(X_train, y_train)\n",
    "    stat = reduce_classify(X_resampled,y_resampled,X_test,y_test,'TL', grid.best_params_,encoder,classifier)\n",
    "    results = results.append(( stat ))\n",
    "\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_classify(X_train,X_test, y_train,y_test, ratios,encoder,classifier):\n",
    "    results = pd.DataFrame()\n",
    "    k_neighbors = [5,10,15]\n",
    "\n",
    "    \n",
    "    resampler_instance = OneSidedSelection()\n",
    "    pipeline = Pipeline([('resampler', resampler_instance),  ('clf', classifier)])\n",
    "    params = {'resampler__sampling_strategy' : ['not minority','not majority','all'],\n",
    "              'resampler__n_neighbors':[1,3,6],\n",
    "             'resampler__n_seeds_S':[10,100,200]}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    grid.fit(X_train, y_train)\n",
    "    pRatio = grid.best_params_['resampler__sampling_strategy']\n",
    "    pNeighbors = grid.best_params_['resampler__n_neighbors']\n",
    "    pSeeds = grid.best_params_['resampler__n_seeds_S']\n",
    "    resampler_instance = OneSidedSelection(sampling_strategy=pRatio,n_neighbors=pNeighbors,n_seeds_S=pSeeds)\n",
    "    X_resampled, y_resampled = resampler_instance.fit_sample(X_train, y_train)\n",
    "    stat = classify(X_resampled,y_resampled,X_test,y_test,'OSS',grid.best_params_ ,'NA','NA',encoder,classifier)\n",
    "    results = results.append(( stat ))\n",
    "\n",
    "    resampler_instance = RandomOverSampler()\n",
    "    pipeline = Pipeline([('resampler', resampler_instance), ('clf', classifier)])\n",
    "    params = {'resampler__sampling_strategy' : ratios}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    grid.fit(X_train, y_train)\n",
    "    pRatio = grid.best_params_['resampler__sampling_strategy']\n",
    "    resampler_instance = RandomOverSampler(sampling_strategy=pRatio)\n",
    "    X_resampled, y_resampled = resampler_instance.fit_sample(X_train, y_train)\n",
    "    stat = classify(X_resampled,y_resampled,X_test,y_test,'ROS', grid.best_params_,'NA','NA',encoder,classifier)\n",
    "    results = results.append(( stat ))\n",
    "\n",
    "    resampler_instance = SMOTE()\n",
    "    pipeline = Pipeline([('resampler', resampler_instance),  ('clf', classifier)])\n",
    "    params = {'resampler__sampling_strategy' : ratios,\n",
    "             'resampler__k_neighbors': k_neighbors}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    grid.fit(X_train, y_train)\n",
    "    pSS = grid.best_params_['resampler__sampling_strategy']\n",
    "    pKN = grid.best_params_['resampler__k_neighbors']\n",
    "    resampler_instance = SMOTE(sampling_strategy=pSS, k_neighbors = pKN)\n",
    "    X_resampled, y_resampled = resampler_instance.fit_sample(X_train, y_train)\n",
    "    stat = classify(X_resampled,y_resampled,X_test,y_test,'SMT', grid.best_params_,'NA','NA',encoder,classifier)\n",
    "    results = results.append(( stat ))\n",
    "    \n",
    "    \n",
    "    resampler_instance = ADASYN()\n",
    "    pipeline = Pipeline([('resampler', resampler_instance), ('clf', classifier)])\n",
    "    params = {'resampler__sampling_strategy' : ratios,\n",
    "             'resampler__n_neighbors': k_neighbors}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    grid.fit(X_train, y_train)\n",
    "    pRatio = grid.best_params_['resampler__sampling_strategy']\n",
    "    pKN = grid.best_params_['resampler__n_neighbors']\n",
    "    resampler_instance = ADASYN(sampling_strategy=pRatio,n_neighbors=pKN)\n",
    "    X_resampled, y_resampled = resampler_instance.fit_sample(X_train, y_train)\n",
    "    stat = classify(X_resampled,y_resampled,X_test,y_test,'ADA', grid.best_params_,'NA','NA',encoder,classifier)\n",
    "    results = results.append(( stat ))\n",
    "    \n",
    "    resampler_instance = BorderlineSMOTE()\n",
    "    pipeline = Pipeline([('resampler', resampler_instance),  ('clf', classifier)])\n",
    "    params = {'resampler__sampling_strategy' : ratios,\n",
    "             'resampler__k_neighbors': k_neighbors,\n",
    "             'resampler__kind':['borderline-1','borderline-2']}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    grid.fit(X_train, y_train)\n",
    "    pSS = grid.best_params_['resampler__sampling_strategy']\n",
    "    pKN = grid.best_params_['resampler__k_neighbors']\n",
    "    pKnd = grid.best_params_['resampler__kind']\n",
    "    resampler_instance = BorderlineSMOTE(sampling_strategy=pSS,k_neighbors=pKN,kind=pKnd)\n",
    "    X_resampled, y_resampled = resampler_instance.fit_sample(X_train, y_train)\n",
    "    stat = classify(X_resampled,y_resampled,X_test,y_test,'SMT_BL',grid.best_params_,'NA','NA',encoder,classifier)\n",
    "    results = results.append(( stat ))\n",
    "    \n",
    "    resampler_instance = SMOTEENN()\n",
    "    pipeline = Pipeline([('resampler', resampler_instance), ('clf', classifier)])\n",
    "    params = {'resampler__sampling_strategy' : ratios}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    grid.fit(X_train, y_train)\n",
    "    pRatio = grid.best_params_['resampler__sampling_strategy']\n",
    "    resampler_instance = SMOTEENN(sampling_strategy=pRatio)\n",
    "    X_resampled, y_resampled = resampler_instance.fit_sample(X_train, y_train)\n",
    "    stat = classify(X_resampled,y_resampled,X_test,y_test,'SMT_ENN', grid.best_params_,'NA','NA',encoder,classifier)\n",
    "    results = results.append(( stat ))\n",
    "    \n",
    "    resampler_instance = SMOTETomek()\n",
    "    pipeline = Pipeline([('resampler', resampler_instance),  ('clf', classifier)])\n",
    "    params = {'resampler__sampling_strategy' : ratios}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    grid.fit(X_train, y_train)\n",
    "    pRatio = grid.best_params_['resampler__sampling_strategy']\n",
    "    resampler_instance = SMOTETomek(sampling_strategy=pRatio)\n",
    "    X_resampled, y_resampled = resampler_instance.fit_sample(X_train, y_train)\n",
    "    stat = classify(X_resampled,y_resampled,X_test,y_test,'SMT_TL', grid.best_params_,'NA','NA',encoder,classifier)\n",
    "    results = results.append(( stat ))\n",
    "    \n",
    "    resampler_instance = RandomUnderSampler()\n",
    "    pipeline = Pipeline([('resampler', resampler_instance),  ('clf', classifier)])\n",
    "    params = {'resampler__sampling_strategy' : ratios}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    grid.fit(X_train, y_train)\n",
    "    pRatio = grid.best_params_['resampler__sampling_strategy']\n",
    "    resampler_instance = RandomUnderSampler(sampling_strategy=pRatio)\n",
    "    X_resampled, y_resampled = resampler_instance.fit_sample(X_train, y_train)\n",
    "    stat = classify(X_resampled,y_resampled,X_test,y_test,'RUS', grid.best_params_,'NA','NA',encoder,classifier)\n",
    "    results = results.append(( stat ))\n",
    "    \n",
    "    \n",
    "    resampler_instance = NeighbourhoodCleaningRule()\n",
    "    pipeline = Pipeline([('resampler', resampler_instance), ('clf', classifier)])\n",
    "    params = {'resampler__sampling_strategy' : ['not minority','not majority','all'],\n",
    "              'resampler__n_neighbors':[3,6,9]}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    grid.fit(X_train, y_train)\n",
    "    pRatio = grid.best_params_['resampler__sampling_strategy']\n",
    "    pN = grid.best_params_['resampler__n_neighbors']\n",
    "    resampler_instance = NeighbourhoodCleaningRule(sampling_strategy=pRatio,n_neighbors=pN)\n",
    "    X_resampled, y_resampled = resampler_instance.fit_sample(X_train, y_train)\n",
    "    stat = classify(X_resampled,y_resampled,X_test,y_test,'NCL',grid.best_params_,'NA','NA',encoder,classifier)\n",
    "    results = results.append(( stat ))\n",
    "\n",
    "    \n",
    "    resampler_instance = TomekLinks()\n",
    "    pipeline = Pipeline([('resampler', resampler_instance),  ('clf', classifier)])\n",
    "    params = {'resampler__sampling_strategy' : ['not minority','not majority','all']}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    grid.fit(X_train, y_train)\n",
    "    pRatio = grid.best_params_['resampler__sampling_strategy']\n",
    "    resampler_instance = TomekLinks(sampling_strategy=pRatio)\n",
    "    X_resampled, y_resampled = resampler_instance.fit_sample(X_train, y_train)\n",
    "    stat = classify(X_resampled,y_resampled,X_test,y_test,'TL', grid.best_params_,'NA','NA',encoder,classifier)\n",
    "    results = results.append(( stat ))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_classify(X_train,y_train,X_test,y_test,resampler_name, parameterR,encoder,classifier):\n",
    "\n",
    "    #take different percentages of columns with 3 steps\n",
    "    n=[math.ceil(len(X_train.columns)*0.7),\n",
    "    math.ceil(len(X_train.columns)*0.5),\n",
    "    math.ceil(len(X_train.columns)*0.3)]\n",
    "\n",
    "    ft_stat = pd.DataFrame()\n",
    "\n",
    "    transformer = 'PCA'\n",
    "    pca_instance = PCA()\n",
    "    pipeline = Pipeline([('transformer', pca_instance), ('clf', classifier)])\n",
    "    params = {'transformer__n_components' : n}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    best_model = grid.fit(X_train, y_train)\n",
    "    parameter_n_comp = grid.best_params_['transformer__n_components']\n",
    "    pca = PCA(n_components=parameter_n_comp)\n",
    "    X_train_principalComponents = pca.fit_transform(X_train)\n",
    "    X_test_principalComponents = pca.fit_transform(X_test)\n",
    "    stats = classify(X_train_principalComponents,y_train,X_test_principalComponents,y_test,resampler_name,parameterR, transformer, grid.best_params_,encoder,classifier)\n",
    "    ft_stat = ft_stat.append(stats)\n",
    "\n",
    "    transformer = 'CFS'\n",
    "    selectKBest_instance = SelectKBest()\n",
    "    pipeline = Pipeline([('transformer', selectKBest_instance),  ('clf', classifier)])\n",
    "    params = {'transformer__k' : n}\n",
    "    grid = GridSearchCV(pipeline, params,cv=5,scoring='roc_auc')\n",
    "    best_model = grid.fit(X_train, y_train)\n",
    "    parameterT = grid.best_params_['transformer__k']\n",
    "    KBest = SelectKBest( k=parameterT)\n",
    "\n",
    "    kbest_selector = KBest.fit(X_train, y_train)\n",
    "    \n",
    "    X_KBestFeatures_train = kbest_selector.transform(X_train)\n",
    "    X_KBestFeatures_test = kbest_selector.transform(X_test)\n",
    "    stats = classify(X_KBestFeatures_train,y_train,X_KBestFeatures_test,y_test,resampler_name,parameterR, transformer, grid.best_params_,encoder,classifier)\n",
    "    ft_stat = ft_stat.append(stats)\n",
    "\n",
    "\n",
    "    return ft_stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "def calculate_vif(X, thresh):\n",
    "    # Taken from https://stats.stackexchange.com/a/253620/53565 and modified\n",
    "    dropped=True\n",
    "    while dropped:\n",
    "        variables = X.columns\n",
    "        dropped = False\n",
    "        vif = [variance_inflation_factor(X[variables].values, X.columns.get_loc(var)) for var in X.columns]\n",
    "        max_vif = max(vif)\n",
    "        if max_vif > thresh:\n",
    "            maxloc = vif.index(max_vif)\n",
    "            print(f'Dropping {X.columns[maxloc]} with vif={max_vif}')\n",
    "            X = X.drop([X.columns.tolist()[maxloc]], axis=1)\n",
    "            dropped=True\n",
    "    return X\n",
    "\n",
    "def removeMulticolinearFeatures(X_train, X_test, vif):\n",
    "    X_train = calculate_vif(X_train, vif)\n",
    "    columns_list = list(X_train)\n",
    "    X_test = X_test[columns_list]\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fmin(objective function, parameters list, optimizing algorithm)\n",
    "#objective function returns a value that has to be minimazied\n",
    "#tpe.suggest runs the Tree-structured Parzen estimator\n",
    "best = 0\n",
    "max_evals = 100\n",
    "def acc_model(params,classifier):\n",
    "    clf = classifier(**params)\n",
    "    return cross_val_score(clf, X_train, y_train,scoring = 'roc_auc').mean()\n",
    "\n",
    "def f_rf(params):\n",
    "    global best\n",
    "    classifier = RandomForestClassifier\n",
    "    acc = acc_model(params,classifier)\n",
    "    if acc > best:\n",
    "        best = acc\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "def getRF():\n",
    "    criterion_list = [\"gini\", \"entropy\"]\n",
    "    max_features_list = ['auto','sqrt','log2','None']\n",
    "    param_space = {\n",
    "        'max_depth': hp.choice('max_depth', range(1,30)),\n",
    "        'max_features': hp.choice('max_features', ['sqrt','log2','None']),\n",
    "        'n_estimators': hp.choice('n_estimators', range(50,150)),\n",
    "        'criterion': hp.choice('criterion', [\"gini\", \"entropy\"])\n",
    "    }\n",
    "        \n",
    "    trials = Trials()\n",
    "    best = fmin(f_rf, param_space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "    print ('best:')\n",
    "    print (best)  \n",
    "    criterion = criterion_list[best['criterion']]\n",
    "    if best['max_depth'] == 0:\n",
    "        max_depth = None\n",
    "    else: \n",
    "        max_depth = best['max_depth']\n",
    "    max_features = max_features_list[best['max_features']]\n",
    "    n_estimators = best['n_estimators']\n",
    "    optimizedRF = RandomForestClassifier(criterion = criterion, max_depth=max_depth,max_features = max_features,n_estimators=n_estimators)\n",
    "    return optimizedRF,best\n",
    "\n",
    "def f_lr(params):\n",
    "    global best\n",
    "    classifier = LogisticRegression\n",
    "    acc = acc_model(params,classifier)\n",
    "    if acc > best:\n",
    "        best = acc\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "def getLR():\n",
    "    penalty_list = ['l1','l2','elasticnet','none']\n",
    "    param_space = {\n",
    "        'C' : hp.uniform('C', 0.001, 100),\n",
    "        'penalty' : hp.choice('penalty', ['l1','l2','elasticnet','none']) \n",
    "    }\n",
    "        \n",
    "    trials = Trials()\n",
    "    best = fmin(f_lr, param_space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "    print ('best:')\n",
    "    print (best)  \n",
    "    penalty = penalty_list[best['penalty']]\n",
    "    C = best['C']\n",
    "    optimizedLR = LogisticRegression(C=C,penalty=penalty)\n",
    "    return optimizedLR,best\n",
    "\n",
    "def f_mlp(params):\n",
    "    global best\n",
    "    classifier = MLPClassifier\n",
    "    acc = acc_model(params,classifier)\n",
    "    if acc > best:\n",
    "        best = acc\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "def getMLP():\n",
    "    param_space = {\n",
    "        'alpha': hp.uniform('alpha',0.0001,1),\n",
    "        'hidden_layer_sizes': hp.randint('hidden_layer_sizes',100)\n",
    "    }\n",
    "    trials = Trials()\n",
    "    best = fmin(f_mlp, param_space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "    print ('best:')\n",
    "    print (best)\n",
    "    alpha = best['alpha']\n",
    "    hidden_layer_sizes = best['hidden_layer_sizes']\n",
    "    optimizedMLP = MLPClassifier(hidden_layer_sizes=(hidden_layer_sizes),\n",
    "                                 alpha=alpha)\n",
    "    return optimizedMLP,best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preprocess_technique_stat = pd.DataFrame()\n",
    "dataset_name = 'LD'\n",
    "ld = pd.read_csv (r'C:\\Users\\ihorh\\Desktop\\Master Thesis\\data_ihorirene\\Thomas (2002)\\Loan Data.csv',delimiter=';')\n",
    "\n",
    "#drop duplicates\n",
    "ld = ld.drop_duplicates(keep='first')\n",
    "\n",
    "# #split dataset into data and target\n",
    "y = ld.BAD\n",
    "X = ld.drop('BAD',axis=1)\n",
    "\n",
    "\n",
    "#data set specific ratios to resample\n",
    "ratios =[0.406,0.452,0.5]\n",
    "\n",
    "toScaleColumns = ['YOB','DAINC','DHVAL','DMORT','DOUTM','DOUTL','DOUTHP','DOUTCC','SINC']\n",
    "\n",
    "X_train_original, X_test_original, y_train_original, y_test_original = split(X,y)\n",
    "\n",
    "#treat missing values\n",
    "imp_cat = SimpleImputer(strategy=\"most_frequent\")\n",
    "imp_num = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X_train_num = X_train_original.select_dtypes(include=['int64', 'float64'])\n",
    "X_test_num = X_test_original.select_dtypes(include=['int64', 'float64'])\n",
    "X_train_cat = X_train_original.select_dtypes(exclude=['int64', 'float64'])\n",
    "X_test_cat = X_test_original.select_dtypes(exclude=['int64', 'float64'])\n",
    "\n",
    "#impute mean mode\n",
    "X_train_cat = pd.DataFrame(imp_cat.fit_transform(X_train_cat),columns=X_train_cat.columns)\n",
    "X_test_cat = pd.DataFrame(imp_cat.transform(X_test_cat),columns=X_test_cat.columns)\n",
    "X_train_num = pd.DataFrame(imp_num.fit_transform(X_train_num),columns = X_train_num.columns)\n",
    "X_test_num = pd.DataFrame(imp_num.transform(X_test_num),columns=X_test_num.columns)\n",
    "\n",
    "X_train_original = pd.concat([X_train_cat,X_train_num],axis=1)\n",
    "X_test_original = pd.concat([X_test_cat,X_test_num],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#scale\n",
    "X_train_original[toScaleColumns] = scaler.fit_transform(X_train_original[toScaleColumns] )\n",
    "X_test_original[toScaleColumns]  = scaler.transform(X_test_original[toScaleColumns] )\n",
    "\n",
    "#downcast numerical columns\n",
    "for column in toScaleColumns:\n",
    "    X_train_original[column] = pd.to_numeric(X_train_original[column], downcast='float')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###ONE HOT ENCODING-------------------------------------------------\n",
    "X_train = one_hot_encode(X_train_original)\n",
    "X_test = one_hot_encode(X_test_original)\n",
    "y_train = y_train_original\n",
    "y_test = y_test_original\n",
    "columns_list = X_train.columns.tolist()\n",
    "X_test = X_test[X_test.columns.intersection(columns_list)]\n",
    "\n",
    "\n",
    "#add columns that are not present in test\n",
    "for column in columns_list:\n",
    "    if column not in X.columns:\n",
    "        X_test[column] = 0\n",
    "\n",
    "\n",
    "#reduce colinearity\n",
    "X_train, X_test = removeMulticolinearFeatures(X_train,X_test, 10)\n",
    "\n",
    "\n",
    "#classifier optimization is performed on OHE dataset\n",
    "RF, bestParametersRF = getRF()\n",
    "LR, bestParametersLR= getLR()\n",
    "MLP, bestParametersMLP = getMLP()\n",
    "classifiers = [LR,RF,MLP]\n",
    "bestParametersList=[str(bestParametersLR),str(bestParametersRF),str(bestParametersMLP)]\n",
    "\n",
    "\n",
    "i = 0\n",
    "for classifier in classifiers:\n",
    "    stat = classify_master(X_train, X_test, y_train, y_test,'OHE',classifier)\n",
    "    stat['ClassifierParameters'] = bestParametersList[i]\n",
    "    preprocess_technique_stat = preprocess_technique_stat.append(stat)\n",
    "    i=i+1\n",
    "\n",
    "\n",
    "###END ONE HOT ENCODING-------------------------------------------------\n",
    "\n",
    "###WOE ENCODING-----------------------------------------------\n",
    "X_train = X_train_original\n",
    "X_test = X_test_original\n",
    "y_train = y_train_original\n",
    "y_test = y_test_original\n",
    "\n",
    "#Encode WOE\n",
    "columns_list = X_train.columns.tolist()\n",
    "for column in columns_list:\n",
    "    if is_string_dtype(X_train[column]):\n",
    "        encoder = ce.WOEEncoder(cols=[column])\n",
    "        encoder.fit(X_train,y_train)\n",
    "        X_train = encoder.transform(X_train)\n",
    "        X_test = encoder.transform(X_test)\n",
    "        \n",
    "#reduce colinearity\n",
    "X_train, X_test = removeMulticolinearFeatures(X_train,X_test, 10)\n",
    "\n",
    "\n",
    "\n",
    "# #classifier optimization is performed on WOE dataset\n",
    "RF, bestParametersRF = getRF()\n",
    "LR, bestParametersLR= getLR()\n",
    "MLP, bestParametersMLP = getMLP()\n",
    "classifiers = [LR,RF,MLP]\n",
    "bestParametersList=[str(bestParametersLR),str(bestParametersRF),str(bestParametersMLP)]\n",
    "\n",
    "\n",
    "i = 0\n",
    "for classifier in classifiers:\n",
    "    stat = classify_master(X_train, X_test, y_train, y_test,'WOE',classifier)\n",
    "    stat['ClassifierParameters'] = bestParametersList[i]\n",
    "    preprocess_technique_stat = preprocess_technique_stat.append(stat)\n",
    "    i=i+1\n",
    "\n",
    "###EMB---------------------------------\n",
    "X_train = X_train_original\n",
    "y_train = y_train_original\n",
    "X_test = X_test_original\n",
    "y_test = y_test_original\n",
    "\n",
    "\n",
    "        \n",
    "embedding_info = cem.get_embedding_info(X_train)\n",
    "\n",
    "X_encoded,encoders = cem.get_label_encoded_data(X_train)\n",
    "\n",
    "\n",
    "\n",
    "embeddings = cem.get_embeddings(X_encoded, y_train, categorical_embedding_info=embedding_info, \n",
    "                            is_classification=True, epochs=100,batch_size=256)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train = cem.fit_transform(X_train, embeddings=embeddings, encoders=encoders, drop_categorical_vars=True)\n",
    "X_test = cem.fit_transform(X_test, embeddings=embeddings, encoders=encoders, drop_categorical_vars=True)\n",
    " \n",
    "\n",
    "#reduce colinearity\n",
    "X_train, X_test = removeMulticolinearFeatures(X_train,X_test, 10)\n",
    "\n",
    "# #classifier optimization is performed on OHE dataset\n",
    "RF, bestParametersRF = getRF()\n",
    "LR, bestParametersLR= getLR()\n",
    "MLP, bestParametersMLP = getMLP()\n",
    "classifiers = [LR,RF,MLP]\n",
    "bestParametersList=[str(bestParametersLR),str(bestParametersRF),str(bestParametersMLP)]\n",
    "\n",
    "i = 0\n",
    "for classifier in classifiers:\n",
    "    stat = classify_master(X_train, X_test, y_train, y_test,'EMB',classifier)\n",
    "    stat['ClassifierParameters'] = bestParametersList[i]\n",
    "    preprocess_technique_stat = preprocess_technique_stat.append(stat)\n",
    "    i=i+1\n",
    "    \n",
    "###END EMB-------------------------------------------------------------\n",
    "preprocess_technique_stat.to_excel(r'C:\\Users\\ihorh\\Desktop\\Master Thesis\\stats\\prep_technique_stats_'+dataset_name+'.xlsx')  \n",
    "   \n",
    "l_stat = preprocess_technique_stat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_technique_stat = pd.DataFrame()\n",
    "dataset_name = 'HMEQ'\n",
    "hmeq = pd.read_csv (r'C:\\Users\\ihorh\\Desktop\\Master Thesis\\data_ihorirene\\hmeq\\hmeq.csv'\n",
    "                 ,delimiter=';')\n",
    "\n",
    "#drop duplicates\n",
    "hmeq = hmeq.drop_duplicates(keep='first')\n",
    "\n",
    "\n",
    "#convert to month scale\n",
    "hmeq[['CLAGE']] = round(hmeq[['CLAGE']]*12,1)\n",
    "\n",
    "\n",
    "#split dataset into data and target\n",
    "y = hmeq.BAD\n",
    "X = hmeq.drop('BAD',axis=1)\n",
    "\n",
    "\n",
    "#data set specific ratios to resample\n",
    "ratios =[0.333,0.416,0.5]\n",
    "\n",
    "toScaleColumns = ['LOAN','MORTDUE','VALUE','CLAGE','DEBTINC']\n",
    "\n",
    "X_train_original, X_test_original, y_train_original, y_test_original = split(X,y)\n",
    "\n",
    "#treat missing values\n",
    "imp_cat = SimpleImputer(strategy=\"most_frequent\")\n",
    "imp_num = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X_train_num = X_train_original.select_dtypes(include=['int64', 'float64'])\n",
    "X_test_num = X_test_original.select_dtypes(include=['int64', 'float64'])\n",
    "X_train_cat = X_train_original.select_dtypes(exclude=['int64', 'float64'])\n",
    "X_test_cat = X_test_original.select_dtypes(exclude=['int64', 'float64'])\n",
    "\n",
    "#impute mean mode\n",
    "X_train_cat = pd.DataFrame(imp_cat.fit_transform(X_train_cat),columns=X_train_cat.columns)\n",
    "X_test_cat = pd.DataFrame(imp_cat.transform(X_test_cat),columns=X_test_cat.columns)\n",
    "X_train_num = pd.DataFrame(imp_num.fit_transform(X_train_num),columns = X_train_num.columns)\n",
    "X_test_num = pd.DataFrame(imp_num.transform(X_test_num),columns=X_test_num.columns)\n",
    "\n",
    "X_train_original = pd.concat([X_train_cat,X_train_num],axis=1)\n",
    "X_test_original = pd.concat([X_test_cat,X_test_num],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#scale\n",
    "X_train_original[toScaleColumns] = scaler.fit_transform(X_train_original[toScaleColumns] )\n",
    "X_test_original[toScaleColumns]  = scaler.transform(X_test_original[toScaleColumns] )\n",
    "\n",
    "#downcast numerical columns\n",
    "for column in toScaleColumns:\n",
    "    X_train_original[column] = pd.to_numeric(X_train_original[column], downcast='float')\n",
    "\n",
    "\n",
    "###ONE HOT ENCODING-------------------------------------------------\n",
    "X_train = one_hot_encode(X_train_original)\n",
    "X_test = one_hot_encode(X_test_original)\n",
    "y_train = y_train_original\n",
    "y_test = y_test_original\n",
    "columns_list = X_train.columns.tolist()\n",
    "X_test = X_test[X_test.columns.intersection(columns_list)]\n",
    "\n",
    "\n",
    "#add columns that are not present in test\n",
    "for column in columns_list:\n",
    "    if column not in X.columns:\n",
    "        X_test[column] = 0\n",
    "\n",
    "\n",
    "#reduce colinearity\n",
    "X_train, X_test = removeMulticolinearFeatures(X_train,X_test, 10)\n",
    "\n",
    "\n",
    "#classifier optimization is performed on OHE dataset\n",
    "RF, bestParametersRF = getRF()\n",
    "LR, bestParametersLR= getLR()\n",
    "MLP, bestParametersMLP = getMLP()\n",
    "classifiers = [LR,RF,MLP]\n",
    "bestParametersList=[str(bestParametersLR),str(bestParametersRF),str(bestParametersMLP)]\n",
    "\n",
    "\n",
    "i = 0\n",
    "for classifier in classifiers:\n",
    "    stat = classify_master(X_train, X_test, y_train, y_test,'OHE',classifier)\n",
    "    stat['ClassifierParameters'] = bestParametersList[i]\n",
    "    preprocess_technique_stat = preprocess_technique_stat.append(stat)\n",
    "    i=i+1\n",
    "\n",
    "\n",
    "###END ONE HOT ENCODING-------------------------------------------------\n",
    "\n",
    "\n",
    "###WOE ENCODING-----------------------------------------------\n",
    "X_train = X_train_original\n",
    "X_test = X_test_original\n",
    "y_train = y_train_original\n",
    "y_test = y_test_original\n",
    "\n",
    "#Encode WOE\n",
    "columns_list = X_train.columns.tolist()\n",
    "for column in columns_list:\n",
    "    if is_string_dtype(X_train[column]):\n",
    "        encoder = ce.WOEEncoder(cols=[column])\n",
    "        encoder.fit(X_train,y_train)\n",
    "        X_train = encoder.transform(X_train)\n",
    "        X_test = encoder.transform(X_test)\n",
    "        \n",
    "#reduce colinearity\n",
    "X_train, X_test = removeMulticolinearFeatures(X_train,X_test, 10)\n",
    "\n",
    "\n",
    "\n",
    "# #classifier optimization is performed on WOE dataset\n",
    "RF, bestParametersRF = getRF()\n",
    "LR, bestParametersLR= getLR()\n",
    "MLP, bestParametersMLP = getMLP()\n",
    "classifiers = [LR,RF,MLP]\n",
    "bestParametersList=[str(bestParametersLR),str(bestParametersRF),str(bestParametersMLP)]\n",
    "\n",
    "\n",
    "i = 0\n",
    "for classifier in classifiers:\n",
    "    stat = classify_master(X_train, X_test, y_train, y_test,'WOE',classifier)\n",
    "    stat['ClassifierParameters'] = bestParametersList[i]\n",
    "    preprocess_technique_stat = preprocess_technique_stat.append(stat)\n",
    "    i=i+1\n",
    "\n",
    "###EMB---------------------------------\n",
    "X_train = X_train_original\n",
    "y_train = y_train_original\n",
    "X_test = X_test_original\n",
    "y_test = y_test_original\n",
    "\n",
    "\n",
    "        \n",
    "embedding_info = cem.get_embedding_info(X_train)\n",
    "\n",
    "X_encoded,encoders = cem.get_label_encoded_data(X_train)\n",
    "\n",
    "\n",
    "\n",
    "embeddings = cem.get_embeddings(X_encoded, y_train, categorical_embedding_info=embedding_info, \n",
    "                            is_classification=True, epochs=100,batch_size=256)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train = cem.fit_transform(X_train, embeddings=embeddings, encoders=encoders, drop_categorical_vars=True)\n",
    "X_test = cem.fit_transform(X_test, embeddings=embeddings, encoders=encoders, drop_categorical_vars=True)\n",
    " \n",
    "\n",
    "#reduce colinearity\n",
    "X_train, X_test = removeMulticolinearFeatures(X_train,X_test, 10)\n",
    "\n",
    "# #classifier optimization is performed on OHE dataset\n",
    "RF, bestParametersRF = getRF()\n",
    "LR, bestParametersLR= getLR()\n",
    "MLP, bestParametersMLP = getMLP()\n",
    "classifiers = [LR,RF,MLP]\n",
    "bestParametersList=[str(bestParametersLR),str(bestParametersRF),str(bestParametersMLP)]\n",
    "\n",
    "i = 0\n",
    "for classifier in classifiers:\n",
    "    stat = classify_master(X_train, X_test, y_train, y_test,'EMB',classifier)\n",
    "    stat['ClassifierParameters'] = bestParametersList[i]\n",
    "    preprocess_technique_stat = preprocess_technique_stat.append(stat)\n",
    "    i=i+1\n",
    "    \n",
    "###END EMB-------------------------------------------------------------\n",
    "preprocess_technique_stat.to_excel(r'C:\\Users\\ihorh\\Desktop\\Master Thesis\\stats\\prep_technique_stats_'+dataset_name+'.xlsx')  \n",
    "   \n",
    "h_stat = preprocess_technique_stat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_technique_stat = pd.DataFrame()\n",
    "dataset_name = 'bene'\n",
    "\n",
    "#data set specific ratios to resample\n",
    "ratios =[0.166,0.332,0.5]\n",
    "bene = pd.read_csv (r'C:\\Users\\ihorh\\Desktop\\Master Thesis\\data_ihorirene\\bene3\\data_bene3.csv'\n",
    "                 ,delimiter=';')\n",
    "#drop duplicates\n",
    "bene = bene.drop_duplicates(keep='first')\n",
    "\n",
    "\n",
    "bene = bene.drop(columns=['account_id'])\n",
    "bene = bene.rename(columns={'default': 'BAD'})\n",
    "\n",
    "\n",
    "#treat values based on their meaning\n",
    "bene.loc[bene.HWGV_REL_MD_AT > 900, 'HWGV_REL_MD_AT'] = np.nan\n",
    "\n",
    "bene.replace({'HGESL_KD' : { 2: 'm',1: 'f'}},inplace=True)\n",
    "bene.loc[bene.HWOON_STA_MD_AT == 99998, 'HWOON_STA_MD_AT'] = bene.HLTD_MD_AT\n",
    "\n",
    "bene.replace({'HBURG_KD' : { 0: 'NA', 1 : 'Divorced', 2 : 'Married',3:'Not married',4:'Legally divorced',5:'Widow',6:'Overleden' }},inplace=True)\n",
    "\n",
    "bene.replace({'HWOON_STA_KD' : {1 : 'Owner', 2 : 'Renter',3:'Lives at parents',4:'Other'}},inplace=True)\n",
    "bene.replace({'HNAT_KD' : {1 : 'Belgium', 2 : 'West-Europe',3:'Rest of Europe',4:'North America/ Australia',5:'South/ Central America',6:'Asia/ Middle East',7:'Africa',8:'Not applicable'}},inplace=True)\n",
    "\n",
    "\n",
    "bene[['HLTD_MD_AT']] = round(bene[['HLTD_MD_AT']]/12,1)\n",
    "bene[['HWOON_STA_MD_AT']] = round(bene[['HWOON_STA_MD_AT']]/12,1)\n",
    "bene[['HWGV_REL_MD_AT']] = round(bene[['HWGV_REL_MD_AT']]/12,1)\n",
    "  \n",
    "bene.rename(columns={'HGESL_KD': 'Gender','HLTD_MD_AT': 'Age','HWOON_STA_MD_AT':'LivesWithParents',\n",
    "                                 'HWGV_REL_MD_AT':'WorksAtEmployer','HNAT_KD':'Origin','HBURG_KD':'CivilState'\n",
    "                                ,'HWOON_STA_KD':'HomeOwnership','MOB':'Month'}, inplace=True)\n",
    "\n",
    "#end treat values based on their meaning\n",
    "\n",
    "# #split dataset into data and target\n",
    "y = bene.BAD\n",
    "X = bene.drop('BAD',axis=1)\n",
    "\n",
    "\n",
    "toScaleColumns = ['amount_of_loan','Month','LivesWithParents','Age','WorksAtEmployer','DPD']\n",
    "\n",
    "\n",
    "X_train_original, X_test_original, y_train_original, y_test_original = split(X,y)\n",
    "\n",
    "#treat missing values\n",
    "imp_cat = SimpleImputer(strategy=\"most_frequent\")\n",
    "imp_num = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X_train_num = X_train_original.select_dtypes(include=['int64', 'float64'])\n",
    "X_test_num = X_test_original.select_dtypes(include=['int64', 'float64'])\n",
    "X_train_cat = X_train_original.select_dtypes(exclude=['int64', 'float64'])\n",
    "X_test_cat = X_test_original.select_dtypes(exclude=['int64', 'float64'])\n",
    "\n",
    "#impute mean mode\n",
    "X_train_cat = pd.DataFrame(imp_cat.fit_transform(X_train_cat),columns=X_train_cat.columns)\n",
    "X_test_cat = pd.DataFrame(imp_cat.transform(X_test_cat),columns=X_test_cat.columns)\n",
    "X_train_num = pd.DataFrame(imp_num.fit_transform(X_train_num),columns = X_train_num.columns)\n",
    "X_test_num = pd.DataFrame(imp_num.transform(X_test_num),columns=X_test_num.columns)\n",
    "\n",
    "X_train_original = pd.concat([X_train_cat,X_train_num],axis=1)\n",
    "X_test_original = pd.concat([X_test_cat,X_test_num],axis=1)\n",
    "\n",
    "\n",
    "#scale\n",
    "X_train_original[toScaleColumns] = scaler.fit_transform(X_train_original[toScaleColumns] )\n",
    "X_test_original[toScaleColumns]  = scaler.transform(X_test_original[toScaleColumns] )\n",
    "\n",
    "#downcast numerical columns\n",
    "for column in toScaleColumns:\n",
    "    X_train_original[column] = pd.to_numeric(X_train_original[column], downcast='float')\n",
    "\n",
    "\n",
    "###ONE HOT ENCODING-------------------------------------------------\n",
    "X_train = one_hot_encode(X_train_original)\n",
    "X_test = one_hot_encode(X_test_original)\n",
    "y_train = y_train_original\n",
    "y_test = y_test_original\n",
    "columns_list = X_train.columns.tolist()\n",
    "X_test = X_test[X_test.columns.intersection(columns_list)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#add columns that are not present in test\n",
    "for column in columns_list:\n",
    "    if column not in X.columns:\n",
    "        X_test[column] = 0\n",
    "\n",
    "\n",
    "#reduce colinearity\n",
    "X_train, X_test = removeMulticolinearFeatures(X_train,X_test, 10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#classifier optimization is performed on OHE dataset\n",
    "RF, bestParametersRF = getRF()\n",
    "LR, bestParametersLR= getLR()\n",
    "MLP, bestParametersMLP = getMLP()\n",
    "classifiers = [LR,RF,MLP]\n",
    "bestParametersList=[str(bestParametersLR),str(bestParametersRF),str(bestParametersMLP)]\n",
    "\n",
    "\n",
    "i = 0\n",
    "for classifier in classifiers:\n",
    "    stat = classify_master(X_train, X_test, y_train, y_test,'OHE',classifier)\n",
    "    stat['ClassifierParameters'] = bestParametersList[i]\n",
    "    preprocess_technique_stat = preprocess_technique_stat.append(stat)\n",
    "    i=i+1\n",
    "\n",
    "\n",
    "###END ONE HOT ENCODING------------------------------------------\n",
    "\n",
    "\n",
    "###WOE ENCODING-----------------------------------------------\n",
    "X_train = X_train_original\n",
    "X_test = X_test_original\n",
    "y_train = y_train_original\n",
    "y_test = y_test_original\n",
    "\n",
    "#Encode WOE\n",
    "columns_list = X_train.columns.tolist()\n",
    "for column in columns_list:\n",
    "    if is_string_dtype(X_train[column]):\n",
    "        encoder = ce.WOEEncoder(cols=[column])\n",
    "        encoder.fit(X_train,y_train)\n",
    "        X_train = encoder.transform(X_train)\n",
    "        X_test = encoder.transform(X_test)\n",
    "        \n",
    "#reduce colinearity\n",
    "X_train, X_test = removeMulticolinearFeatures(X_train,X_test, 10)\n",
    "\n",
    "\n",
    "\n",
    "# #classifier optimization is performed on WOE dataset\n",
    "RF, bestParametersRF = getRF()\n",
    "LR, bestParametersLR= getLR()\n",
    "MLP, bestParametersMLP = getMLP()\n",
    "classifiers = [LR,RF,MLP]\n",
    "bestParametersList=[str(bestParametersLR),str(bestParametersRF),str(bestParametersMLP)]\n",
    "\n",
    "\n",
    "i = 0\n",
    "for classifier in classifiers:\n",
    "    stat = classify_master(X_train, X_test, y_train, y_test,'WOE',classifier)\n",
    "    stat['ClassifierParameters'] = bestParametersList[i]\n",
    "    preprocess_technique_stat = preprocess_technique_stat.append(stat)\n",
    "    i=i+1\n",
    "\n",
    "###EMB---------------------------------\n",
    "X_train = X_train_original\n",
    "y_train = y_train_original\n",
    "X_test = X_test_original\n",
    "y_test = y_test_original\n",
    "\n",
    "\n",
    "        \n",
    "embedding_info = cem.get_embedding_info(X_train)\n",
    "\n",
    "X_encoded,encoders = cem.get_label_encoded_data(X_train)\n",
    "\n",
    "\n",
    "\n",
    "embeddings = cem.get_embeddings(X_encoded, y_train, categorical_embedding_info=embedding_info, \n",
    "                            is_classification=True, epochs=100,batch_size=256)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train = cem.fit_transform(X_train, embeddings=embeddings, encoders=encoders, drop_categorical_vars=True)\n",
    "X_test = cem.fit_transform(X_test, embeddings=embeddings, encoders=encoders, drop_categorical_vars=True)\n",
    " \n",
    "\n",
    "#reduce colinearity\n",
    "X_train, X_test = removeMulticolinearFeatures(X_train,X_test, 10)\n",
    "\n",
    "# #classifier optimization is performed on OHE dataset\n",
    "RF, bestParametersRF = getRF()\n",
    "LR, bestParametersLR= getLR()\n",
    "MLP, bestParametersMLP = getMLP()\n",
    "classifiers = [LR,RF,MLP]\n",
    "bestParametersList=[str(bestParametersLR),str(bestParametersRF),str(bestParametersMLP)]\n",
    "\n",
    "i = 0\n",
    "for classifier in classifiers:\n",
    "    stat = classify_master(X_train, X_test, y_train, y_test,'EMB',classifier)\n",
    "    stat['ClassifierParameters'] = bestParametersList[i]\n",
    "    preprocess_technique_stat = preprocess_technique_stat.append(stat)\n",
    "    i=i+1\n",
    "    \n",
    "###END EMB-------------------------------------------------------------\n",
    "preprocess_technique_stat.to_excel(r'C:\\Users\\ihorh\\Desktop\\Master Thesis\\stats\\prep_technique_stats_'+dataset_name+'.xlsx')  \n",
    "    \n",
    "b_stat = preprocess_technique_stat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_joined = pd.concat([ l_stat,h_stat,b_stat],axis=1)\n",
    "stats_joined.columns =['l_Dataset','l_Encoder','l_Resampler','l_Resampler_Parameters','l_Feature_Selection','l_FS_Parameters','l_Classifier','l_Recall','l_f1','l_AUC','l_BS','l_Classifier_Parameters',\n",
    "                       'h_Dataset','h_Encoder','h_Resampler','h_Resampler_Parameters','h_Feature_Selection','h_FS_Parameters','h_Classifier','h_Recall','h_f1','h_AUC','h_BS','h_Classifier_Parameters',\n",
    "                       'b_Dataset','b_Encoder','b_Resampler','b_Resampler_Parameters','b_Feature_Selection','b_FS_Parameters','b_Classifier','b_Recall','b_f1','b_AUC','b_BS','b_Classifier_Parameters']\n",
    "stats_joined['l_Recall_r'] = stats_joined['l_Recall'].rank(method='dense',ascending=False)\n",
    "stats_joined['l_f1_r'] = stats_joined['l_f1'].rank(method='dense',ascending=False)\n",
    "stats_joined['l_AUC_r'] = stats_joined['l_AUC'].rank(method='dense',ascending=False)\n",
    "stats_joined['l_BS_r'] = stats_joined['l_BS'].rank(method='dense')\n",
    "stats_joined['h_Recall_r'] = stats_joined['h_Recall'].rank(method='dense',ascending=False)\n",
    "stats_joined['h_f1_r'] = stats_joined['h_f1'].rank(method='dense',ascending=False)\n",
    "stats_joined['h_AUC_r'] = stats_joined['h_AUC'].rank(method='dense',ascending=False)\n",
    "stats_joined['h_BS_r'] = stats_joined['h_BS'].rank(method='dense')\n",
    "stats_joined['b_Recall_r'] = stats_joined['b_Recall'].rank(method='dense',ascending=False)\n",
    "stats_joined['b_f1_r'] = stats_joined['b_f1'].rank(method='dense',ascending=False)\n",
    "stats_joined['b_AUC_r'] = stats_joined['b_AUC'].rank(method='dense',ascending=False)\n",
    "stats_joined['b_BS_r'] = stats_joined['b_BS'].rank(method='dense')\n",
    "\n",
    "stats_joined['AverageRank']= stats_joined[['l_Recall_r','l_f1_r','l_AUC_r','l_BS_r',\n",
    "                                         'h_Recall_r','h_f1_r','h_AUC_r','h_BS_r'\n",
    "                                         ,'b_Recall_r','b_f1_r','b_AUC_r','b_BS_r']].sum(axis=1)/12\n",
    "\n",
    "stats_joined.to_excel(r'C:\\Users\\ihorh\\Desktop\\Master Thesis\\stats\\statistics_joined.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
